import os
import json
import torch
import uuid
import logging
from flask import Flask, request, jsonify
from flask_cors import CORS
from transformers import AutoModelForCausalLM, AutoTokenizer, SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan
from dotenv import load_dotenv
import scipy.io.wavfile

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Flask
app = Flask(__name__)
CORS(app)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–µ–π
models = {
    "gpt2": None,
    "speech": None
}

def load_models():
    """–ó–∞–≥—Ä—É–∑–∫–∞ AI –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–∏—Å–∞"""
    try:
        token = os.getenv("HUGGINGFACE_HUB_TOKEN", "")

        logger.info("üîÑ Loading GPT-2 model...")
        # GPT-2 –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        models["gpt2"] = {
            "tokenizer": AutoTokenizer.from_pretrained("gpt2"),
            "model": AutoModelForCausalLM.from_pretrained("gpt2")
        }

        logger.info("‚úÖ GPT-2 loaded")

        logger.info("üîÑ Loading SpeechT5 model...")
        # SpeechT5 –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ—á–∏
        models["speech"] = {
            "processor": SpeechT5Processor.from_pretrained("microsoft/speecht5_tts"),
            "model": SpeechT5ForTextToSpeech.from_pretrained("microsoft/speecht5_tts"),
            "vocoder": SpeechT5HifiGan.from_pretrained("microsoft/speecht5_hifigan")
        }

        logger.info("‚úÖ All models loaded successfully")
        return True

    except Exception as e:
        logger.error(f"‚ùå Error loading models: {str(e)}")
        return False

@app.route('/health', methods=['GET'])
def health():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–∞"""
    return jsonify({
        "status": "ok",
        "service": "llm-api",
        "models_loaded": models["gpt2"] is not None
    })

@app.route('/generate', methods=['POST'])
def generate():
    """–û—Å–Ω–æ–≤–Ω–æ–π endpoint –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏ —Ä–µ—á–∏"""
    try:
        data = request.json

        if not data or 'message' not in data:
            return jsonify({
                "success": False,
                "error": "No message provided"
            }), 400

        message = data.get('message', '')
        user_id = data.get('user_id', 'anonymous')

        logger.info(f"üì® Received message from {user_id}: {message[:50]}...")

        # 1. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é GPT-2
        if not models["gpt2"]:
            return jsonify({
                "success": False,
                "error": "Text model not loaded"
            }), 503

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
        inputs = models["gpt2"]["tokenizer"].encode(message, return_tensors='pt')
        outputs = models["gpt2"]["model"].generate(
            inputs,
            max_length=100,
            num_return_sequences=1,
            temperature=0.7,
            do_sample=True
        )

        generated_text = models["gpt2"]["tokenizer"].decode(outputs[0], skip_special_tokens=True)

        # –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ (—É–¥–∞–ª—è–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã)
        generated_text = generated_text.replace(message, "").strip()
        if not generated_text:
            generated_text = "–Ø –ø–æ–ª—É—á–∏–ª –≤–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. –ß–µ–º –µ—â–µ –º–æ–≥—É –ø–æ–º–æ—á—å?"

        logger.info(f"üìù Generated text: {generated_text[:100]}...")

        # 2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ—á–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        audio_filename = None

        if models["speech"] and data.get('generate_audio', False):
            try:
                audio_filename = f"{uuid.uuid4().hex}.wav"
                audio_path = os.path.join('audio', audio_filename)

                # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ—Ç
                os.makedirs('audio', exist_ok=True)

                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ—á–∏
                speech_inputs = models["speech"]["processor"](
                    generated_text,
                    return_tensors="pt"
                )
                speaker_embeddings = torch.zeros(1, 512)  # –ü—Ä–æ—Å—Ç—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏

                with torch.no_grad():
                    speech = models["speech"]["model"].generate_speech(
                        speech_inputs["input_ids"],
                        speaker_embeddings,
                        vocoder=models["speech"]["vocoder"]
                    )

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞—É–¥–∏–æ—Ñ–∞–π–ª
                scipy.io.wavfile.write(audio_path, 16000, speech.numpy())
                logger.info(f"üîä Audio saved: {audio_filename}")

            except Exception as e:
                logger.error(f"Audio generation error: {str(e)}")
                audio_filename = None

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –æ—Ç–≤–µ—Ç–∞
        response = {
            "success": True,
            "text": generated_text,
            "audio_filename": audio_filename,
            "user_id": user_id,
            "processing_time": 0.5  # –ú–æ–∂–Ω–æ –≤—ã—á–∏—Å–ª—è—Ç—å —Ä–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è
        }

        logger.info(f"‚úÖ Response generated for {user_id}")
        return jsonify(response)

    except Exception as e:
        logger.error(f"‚ùå Error in /generate: {str(e)}")
        return jsonify({
            "success": False,
            "error": str(e),
            "fallback_text": f"–Ø –ø–æ–ª—É—á–∏–ª –≤–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: '{message if 'message' in locals() else ''}'. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
        }), 500

@app.route('/audio/<filename>', methods=['GET'])
def get_audio(filename):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞"""
    try:
        audio_path = os.path.join('audio', filename)

        if not os.path.exists(audio_path):
            return jsonify({"error": "Audio file not found"}), 404

        from flask import send_file
        return send_file(audio_path, mimetype='audio/wav')

    except Exception as e:
        return jsonify({"error": str(e)}), 500

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ
load_models()

if __name__ == '__main__':
    logger.info("üöÄ Starting LLM Service on port 5000...")
    app.run(host='0.0.0.0', port=5000, debug=False)